---
title: "extract Candidate Sequences according to the WHO definition"
author: "Jonas HÃ¼gel"
date: "24.04.2023"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tSPMPlus with MLHO}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In this vignette, we will explore how we can use the tSPM+ package to identify possibible long covid candidate sequences according to the WHO definition. The WHO definition requires 2 important factors for a symptom to be long covid:
* the patient must had covid 3 months ago and the symptons can not explained by other conditions
* the patient had covid 2 months ago and ongoing symptons that cannont explainend by other conditions

We call a condition a candidate, if a sequence exists that starts with covid, end with that condition and has a duration >3 months.
We call a condition a weak candidate, if we have sequence that starts with covid, end with that condition with a duration <=3 months and a sequence covid

For identify 1) we have to identify sequences that start

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Load the required packages and library

```{r load required packages, warning=FALSE}


if(!require(mlho)){
  require(devtools)
  devtools::install_github("hestiri/mlho")
}
if(!require(tSPMPlus)){
#  devtools::install_github("jonashuegel/tSPMPlusR", args="--recursive")
# while the code is not published install your local package version   
}
## load MLHO depenencies
if(!require(pacman)){
  install.packages("pacman")
}
pacman::p_load(data.table, devtools, backports, Hmisc, tidyr,dplyr,ggplot2,plyr,scales,readr,
               httr, DT, lubridate, tidyverse,reshape2,foreach,doParallel,caret,gbm,lubridate,praznik, epitools)

```

## Prepare the data

In this example, we are using the example data mart from the [MLHO](https://github.com/hestiri/mlho) package. It consists of a preprocessed extract from the [`syntheticmass`data set](https://synthea.mitre.org/downloads) generated by [SyntheaTM](https://synthetichealth.github.io/synthea/), an open-source patient population simulation made available by [The MITRE Corporation](https://health.mitre.org/).

The dbmart consist of 4 rows, the patient number (`patient_num`), the phenx (`phenx`), the corresponding description for the phenx (`description`) and last but not least the date when the phenx was recorded (`start_date`).

The dems table contains one row for each patient storing their demographic information.

The labeldt table consist of 2 columns, storing the patient number (`patient_num`) and the corresponding label (`label`) for our machine learning task.

Lets load the tables and take a look at them.

```{r load data}
#load the data

labeldt <- mlho::labeldt
dems <- mlho::dems 
dbMart.alpha <- mlho::dbmart
#lets take a look at each data frame
head(labeldt)
head(dems)
head(dbMart.alpha)
```

To create the transitive sequences we need only the dbmart data frame. The other data frames become important later, when we want to apply MLHO on the sequencing. tSPM+ requires that we use numeric identifiers for the patient number as well as for the identifier of the data mart entries (phenx). Calling the `transformDbMartToNumeric` function will create numeric ids (starting by zero and counting up wards) for the patient numbers and phenx. We are reducing the number of patients for this example, since we might run into memory issues on Laptops with 16GB RAM when later applying the `MSRMR` algorithm from the `MLHO` package.

```{r transform to numeric, warning=FALSE}
dbMart <- tSPMPlus::transformDbMartToNumeric(dbMart.alpha)
dbMart$dbMart <- dbMart$dbMart %>% subset(num_pat_num < 350)
# dbMart contains a list of 3 Elements, the numeric dbMart,
# the patient ID look-up table and the phenx look-up table
head(dbMart$dbMart)
head(dbMart$phenxLookUp)
head(dbMart$patientLookUp)
```

## Call tSPM+

The `tSPM+` package provides multiple functions to extract the temporal sequence. Each of these functions is calling the underlying `tSPMPlus` function with a different set of preconfigured parameters. In this example we want to extract all non-sparse transitive sequences and are therefore using the `extractNonSparseSequences` function.

```{r call tSPM+}
#define function parameters

#modify this path, tSPM+ will write out a binary file for each patient containing the sequence
outDir = "out/"
outputFilePrefix = "tSPMPlus_MLHO_Example"
sparsity = 0.05
require(parallel)
numOfThreads = detectCores()
nonSparseSequences = tSPMPlus::extractNonSparseSequences(as.data.frame(dbMart$dbMart),outDir, outputFilePrefix, sparsity, numOfThreads)
```

If we take a look at the `nonSparseSequences` data frame, we will see that it consists of three columns. One for the patient Id, one for the sequence and for the duration (time diff between the two phenx of the sequence). The sequences are stored as numbers and but are human readable and interpretable. A sequence consists of the both numeric ids of it phenx, where the second sequence was filled with leading 0 up to seven digits. In our example the first non sparse sequence for `patient 0` is `1380000138` and consist of the both phenx with the id `138`. If we want to get the description of the corresponding phenx, we first need to look up the corresponding alphanumeric value ID in `dbMart$phenxLookUp`, before we can extract the corresponding description from the original dbmart.

```{r view sequences}
head(nonSparseSequences)
```

## Use the sequences as input data for MLHO (slightly modified from the [mlho vanilla vignette](https://github.com/hestiri/mlho/blob/master/vignettes/MLHO_Vanilla.Rmd))

Here we go through the vanilla implementation of MLHO using the transitive sequences extracted from the synthetic data provided in the package.

### update the alpha-numeric patient IDs to the numeric one

```{r prepare data for MLHO}

labeldt <- labeldt %>% dplyr::left_join(dbMart$patientLookUp, by="patient_num") %>% dplyr::select(-"patient_num") %>% dplyr::rename_at('num_pat_num', ~'patient_num') %>% subset(patient_num < 500)
dems <- dems %>% dplyr::left_join(dbMart$patientLookUp, by="patient_num") %>% dplyr::select(-"patient_num") %>% dplyr::rename_at('num_pat_num', ~'patient_num') %>% subset(patient_num < 500)


```

###split data into train-test Before we use our non sparse sequences as new dbMart we need to rename the `sequence` column to `phenx`, since the \``mlho::MSMRlite` function expects the phenx column in the dbMart.

```{r create train and test set}
dbmart <- nonSparseSequences %>% rename_at('sequence', ~'phenx')
uniqpats <- c(as.character(unique(dbmart$patient_num)))
#using a 70-30 ratio
test_ind <- sample(uniqpats,
                   round(.3*length(uniqpats)))
test_labels <- subset(labeldt,labeldt$patient_num %in% c(test_ind))
print("test set lables:")
table(test_labels$label)
train_labels <- subset(labeldt,!(labeldt$patient_num %in% c(test_ind)))
print("train set lables:")
table(train_labels$label)
# train and test sets 
dat.train  <- subset(dbmart,!(dbmart$patient_num %in% c(test_ind)))
dat.test <- subset(dbmart,dbmart$patient_num %in% c(test_ind))
```

## Further dimensionality reduction on training set with MSMR lite using JMI

From here, we will split the data into a train and a test set and apply `MSMR.lite` to the training data. It is important that we set `sparsity=NA`, because, we already removed sparse sequences using the more performance tSPM+ implementation.

```{r call MSMRlite for train set}
data.table::setDT(dat.train)
dat.train[,row := .I]
dat.train$value.var <- 1
uniqpats.train <- c(as.character(unique(dat.train$patient_num)))
##here is the application of MSMR.lite
dat.train <- MSMSR.lite(MLHO.dat=dat.train,
                        patients = uniqpats.train,
                        sparsity=NA, 
                        labels = labeldt,
                        topn=200)
```

Notice that we are removing concepts that had prevalence less than 0.5% and then only taking the top 200 after the JMI rankings. See help (`?mlho::MSMSR.lite`) for `MSMR.lite` parameters.

Now we have the training data with the top 200 features, to which we can add demographic features. Now on to prepping the test set:

```{r MSMRlite for test set}
dat.test <- subset(dat.test,dat.test$phenx %in% colnames(dat.train))
setDT(dat.test)
dat.test[,row := .I]
dat.test$value.var <- 1
uniqpats.test <- c(as.character(unique(dat.test$patient_num)))
dat.test <- MSMSR.lite(MLHO.dat=dat.test,patients = uniqpats.test,sparsity=NA,jmi = FALSE,labels = labeldt)
```

Here notice that we only used `MSMR.lite` to generate the wide table that matches to the `dat.train` table.

##Modeling we will use the `mlearn` function to do the modeling, which includes training the model and testing it on the test set.

```{r, warning=FALSE}
model.test <- mlearn(dat.train,
                   dat.test,
                   dems=dems,
                   save.model=FALSE,
                   classifier="gbm",
                   note="mlho_test_run",
                   cv="cv",
                   nfold=5,
                   aoi="prediabetes",
                   multicore=FALSE)
```

Try `?mlho::mlearn` to learn more about `mlearn` parameters. You can select a variety of models. For binray classification tasks, chose from:ORFlog, RRF,gbm,bayesglm,regLogistic,xgbDART,regLogistic, glmnet, bayesglm, nb, svmRadialWeights,avNNet,and ordinalRF. see the list and description of all available [models](https://rdrr.io/cran/caret/man/models.html)

## `mlearn` outputs

outputs of `mlearn` include:

-   model performance metrics: `model.test$ROC`
    -   in model.test AUROC is `r model.test$ROC$roc`
-   final important features, which can be pulled and save from `model.test$features`
-   absolute errors for each patients using `model.test$AE`
-   missing important features `model.test$missing.features`

For instance, only the following `r nrow(model.test$features)` features were used in phenotyping the `prediabetes`:

```{r}
model.test$features
```

*We recommend iterating the training and testing and storing the coefficients in a directory.*

## Visualizing the outputs

Here we create a plot of the feature importance scores for each of the top (here we have `r nrow(model.test$coefficients)`) predictors identified by MLHO.

To do so, let's map the sequences to their "English" translation. *That's why we kept that 4th column called `description` in `dbmart` and created the phenxLookUp table* But before we can do that we need to extract the single concepts codes from the sequences.

```{r transform numerical sequences}
#Inefficient, TODO replace with better option
dbmart.concepts <- dbMart.alpha[!duplicated(paste0(dbMart.alpha$phenx)), c("phenx","DESCRIPTION")]

transformSeqToString <- function(seq){
  if(grepl("\\D", seq)){
    out <- seq
  }else{
    phenxLenght = 7
    endfirst = nchar(seq) - phenxLenght
    phenx1 <- as.integer(substring(seq, 1,endfirst))
    phenx2 <- as.integer(substring(seq, endfirst + 1))
    phenx <- setNames(as.data.frame(c(phenx1,phenx2)), c("num_Phenx"))
    phenx <- dplyr::left_join(phenx, dbMart$phenxLookUp, by="num_Phenx")
    phenx <- dplyr::left_join(phenx, dbmart.concepts, by="phenx")
    out <- paste(phenx$DESCRIPTION[1],phenx$DESCRIPTION[2], sep = "->")
  }
  return (out)
}

for(i in 1:length(model.test$features$features)){
  model.test$features$features[i] = transformSeqToString(model.test$features$features[i])
}
datatable(dplyr::select(model.test$features,features,`Feature importance`=Overall), options = list(pageLength = 5), filter = 'bottom')
```



Now let us visualize the feature importance:
```{r,fig.align='center',fig.width=7,fig.height=7}
(plot<- ggplot(model.test$features) +
    geom_segment(
      aes(y = 0,
          x = reorder(features,Overall),
          yend = Overall,
          xend = features),
      size=0.5,alpha=0.5) +
    geom_point(
      aes(x=reorder(features,Overall),y=Overall),
      alpha=0.5,size=2,color="red") +
    theme_minimal()+
   coord_flip()+
    labs(y="Feature importance",x=""))
```

